\section*{\uppercase{Appendix}}
\label{sec:appendix}
We show the implementation details of the random-based method and the rule-based method for penetration testing here. Based on the same environments as the standard DQN and our method, these two methods are executed as baselines against our method to compare penetration testing performance in reward and cost awareness.



\subsection*{Random-Based Method}
We implement the random method as a naive baseline that selects actions uniformly and randomly from the action space, without using observations or learning. As shown in Algorithm~\ref{alg:random-agent}, it interacts with the environment for a fixed number of training steps per episode, logging cumulative reward, cost, and goal success. This setup provides a memoryless baseline for benchmarking our Constrained-DQN.

\begin{algorithm}[ht]
\small
\caption{Random-Based Method for Penetration Testing}
\label{alg:random-agent}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{Environment $\mathcal{E}$, total training steps $T$}
\Output{Episode trajectories with action logs and goal status}

Initialize step counter $t \gets 0$, episode index $k \gets 1$\;
\While{$t < T$}{
    Reset environment, get initial observation $o_0$\;
    Initialize episode return $R_k \gets 0$, cost $C_k \gets 0$\;
    
    \While{episode not terminated and $t < T$}{
        Sample action $a_t \sim \mathcal{U}(\mathcal{A})$ \tcp*{Random selection over action space}
        
        Execute $a_t$ in $\mathcal{E}$, observe reward $r_t$, next obs $o_{t+1}$\;
        $R_k \gets R_k + r_t$, $C_k \gets C_k + 1$, $t \gets t + 1$\;
        $o_t \gets o_{t+1}$\;
    }
    Record action sequence, $R_k$, $C_k$, and goal completion flag\;
    $k \gets k + 1$\;
}
\end{algorithm}


\subsection*{Rule-Based Method}
We implement the rule-based penetration testing method that utilizes the decision strategy logic inspired by practical penetration testing procedures, shown in Algorithm~\ref{alg:rule-based}. It reflects staged techniques commonly used in tools such as the Metasploit Framework in conjunction with popular penetration testing tools (e.g., Nmap, Nessus, OpenVAS, Armitage, Wireshark, and Netcat), which start with network and service scan and then proceed to exploitation and privilege escalation~\cite{Ralph2019pentestIOT,Raj2020Metasploit,Malkapurapu2023Metasploit,jeff2024penOverview,Skandylas2025pentest}.


The rule-based method tracks the host state by parsing observations and prioritizes scans (e.g., process, OS, service, and subnet scans). After that, attempting exploits or privilege escalation will be executed. We define failed exploits as failed penetration after a retry limit, which is set to a maximum of 2 to prevent repeated attempts on ineffective targets. Meanwhile, if one action cannot satisfy the strategy logic (i.e., no exploits and no need for privilege escalation), we return it to a \textit{No-op}. This heuristic strategy provides a deterministic and interpretable baseline for evaluating our approach.


\begin{algorithm}[ht]
\small
\caption{Rule-Based Method for Penetration Testing}
\label{alg:rule-based}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{Environment $\mathcal{E}$, total training steps $T$}
\Output{Episode logs with action traces and goal status}

Initialize step counter $t \gets 0$, episode index $k \gets 1$\;
\While{$t < T$}{
    Reset environment and internal agent state\;
    Receive initial observation $o_0$\;
    Initialize episode return $R_k \gets 0$, cost $C_k \gets 0$\;

    \While{episode not terminated and $t < T$}{
        Parse state vector to extract host-level information (e.g., reachability, access, services)\;

        \tcp{Hierarchical rule-based action selection}
        \If{process or OS scan is available}{
            Select action $a_t \gets$ process/OS scan
        }
        \ElseIf{service scan is available}{
            Select action $a_t \gets$ service scan
        }
        \ElseIf{subnet scan is available}{
            Select action $a_t \gets$ subnet scan
        }
        \ElseIf{exploitable service is known and retry budget not exceeded}{
            Select action $a_t \gets$ exploit
        }
        \ElseIf{privilege escalation opportunity is found}{
            Select action $a_t \gets$ privilege escalation
        }
        \ElseIf{secondary process/OS or service or subnet scan is still available}{
            Retry scans in the same priority order
        }
        \Else{
            Select fallback scan or \textit{No-op} action $a_t$
        }

        Execute $a_t$ in $\mathcal{E}$, observe $r_t$, next observation $o_{t+1}$, and info\;
        Update internal state (e.g., access levels, exploit attempts, scan coverage)\;
        $R_k \gets R_k + r_t$, $C_k \gets C_k + 1$, $t \gets t + 1$\;
        $o_t \gets o_{t+1}$\;
    }
    Log action sequence, text trace, and goal outcome for episode $k$\;
    $k \gets k + 1$\;
}
\end{algorithm}

