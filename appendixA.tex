\section{\uppercase{Penetration Testing Environment}}
\label{sec:penEnv}


We demonstrate a simplified example of constrained policy optimization on a multi-objective RL framework to introduce our penetration testing environment, which aims to maximize benefits while being cost-aware.

\begin{table*}[t]
\centering
\caption{Network zone communication rules with firewall permission.}
\label{tab:network_policies}
\begin{tabular}{llp{8cm}}
\toprule
\textbf{Code} & \textbf{Direction} & \textbf{Description} \\
\midrule
\textbf{E2D} & External $\rightarrow$ DMZ & Only \texttt{http} and \texttt{ssh} services are permitted into the DMZ, exposing limited public-facing functionality. \\
\textbf{D2E} & DMZ $\rightarrow$ External & All services are allowed in the internal network. \\
\textbf{D2I} & DMZ $\rightarrow$ Internal User & Direct communication is blocked to prevent lateral movement without escalation. \\
\textbf{I2D} & Internal User $\rightarrow$ DMZ & Internal users can access the DMZ via \texttt{ssh} for secure administrative tasks. \\
\textbf{I2P} & Internal User $\rightarrow$ Penetration Target & Users connect using \texttt{ssh} and \texttt{ftp}; \texttt{ftp} is unrestricted to support simulated data movement. \\
\textbf{P2I} & Penetration Target $\rightarrow$ Internal User & Permissions mirror those of \textbf{I2P}. \\
\textbf{D2P} & DMZ $\rightarrow$ Penetration Target & \texttt{ssh} and \texttt{http} are blocked; \texttt{ftp} is allowed for limited file transfer and simulated exfiltration. \\
\textbf{P2D} & Penetration Target $\rightarrow$ DMZ & Permissions mirror those of \textbf{D2P}. \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Environment Components}
In this paper, the RL agent employs DQN techniques, enhanced with experience replay and $\epsilon$-greedy exploration. Constrained agents extend this approach into a multi-objective setting by jointly optimizing task performance and operational cost, demonstrating the feasibility of RL in domains where side effects (e.g., detection risk or time cost) must be explicitly managed. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{webist_Conference_Latex/figures/runningExam/runningEx_network.png}
    \caption{Network layout of the simulated penetration testing environment. The Demilitarized Zone (DMZ) is externally accessible and hosts multiple vulnerable services. The host highlighted in \textbf{bold} in the Penetration Targets zone is the final target and will be highly rewarded once compromised. 
    }
    \label{fig:networkConfig}
\end{figure}


As illustrated in Figure~\ref{fig:networkConfig}, we set up firewall-allowed permissions and blocked communication rules to simulate the penetration testing. In our penetration testing environment, five public-facing services are available, including: \texttt{http} (web server, port 80), \texttt{ssh} (secure shell, port 22), \texttt{ftp} (file transfer protocol, port 21), \texttt{tomcat} (Apache servlet container), and \texttt{dac} (discretionary access control list service, a Windows-specific privilege escalation vector). While present on user hosts, the \texttt{dac} service is not a network-facing protocol. It is exploited locally for privilege escalation simulation. 

Meanwhile, a segmented network architecture comprising four major zones is adopted: the External Internet (untrusted), the Demilitarized Zone (DMZ), the Internal Network (trusted), and the Penetration Target zone. The DMZ acts as a buffer and hosts public-facing services (e.g., web server, SSH, Tomcat) while isolating internal resources from direct exposure to external exposure. In our simulation, the DMZ contains a Linux-based host with public access. For the final target of our simulation, we set a target host with sensitive information in the Penetration Target zone.

Moreover, we set up firewall-allowed permissions and service-blocked rules to limit unnecessary exposure across zones and simulate penetration testing. As shown in Figure~\ref{fig:networkConfig}, \textbf{Svc} denotes a service permitted by firewall rules. Permissions in paths are introduced in Table~\ref{tab:network_policies}.

These configurations simulate a layered security architecture requiring multi-step privilege escalation and lateral movement to reach sensitive targets.


\subsection{Penetration Testing in RL: State, Action, Reward, and Cost}

In the simulated environment, each interaction is recorded as a sequence of (\textit{state, action, reward, cost}), with each episode forming an \emph{penetration path}. Over time, the agent learns which sequences of actions produce the maximum cumulative rewards while minimizing costs.

\noindent \textit{\textbf{Action}}: 
The agent operates in discrete action spaces encoded as integers. In our simulated environment, there are five types of actions, including:
\begin{itemize}
    \item \textbf{Scan:} Probes a host to enumerate available services or discover reachability.
    \item \textbf{Exploit:} Attempts to compromise a specific service with a known vulnerability.
    \item \textbf{Privilege Escalation:} Gains higher-level access on an already compromised host.
    \item \textbf{Pivot:} Moves laterally from one host to another, leveraging existing access.
    \item \textbf{No-op:} A passive action is taken when no meaningful or safe operations are available.
\end{itemize}

\noindent \textit{\textbf{State}}:
The state represents the basic configuration of the network and determines how actions affect the environment and what agents will subsequently observe.

The simulation environment's internal state includes host connections, services, privileges, and compromise status. It will be updated after the agent takes action. Based on the new state and the outcome of the actions (rewards and costs), an observation will be generated and returned to the agent as input to the agent’s decision-making policy. It will contain indicators of discovered services, compromised hosts, current privilege levels, and auxiliary flags (e.g., success, permission denied, or connection errors). 


In our penetration testing environment, the agent does not have direct access to the environment’s full internal state. Instead, it receives the observation after each action. The agent learns observations to infer hidden aspects of the environment, prioritizes strategic actions, and adapts its policy to maximize reward while minimizing operational costs. This separation of state and observation allows us to simulate both fully observable and partially observable settings by limiting or filtering information passed from the true state to the agent.

\noindent \textit{\textbf{Reward and Cost}}:
Agent actions are selected by maximizing a scalarized Q-value that incorporates expected reward and cost. The environment computes both rewards and costs dynamically based on the agent's current state and chosen action. A non-zero reward is assigned for meaningful transitions (e.g., successful exploitation of a vulnerability, privilege escalation, or compromise of the final target host). 

Each action incurs a fixed cost of 1, which accumulates over the episode. The cost-aware agent learns the cost value with TD updates. This cost signal is incorporated into the constraint policy to balance reward maximization and adherence to a specified cost constraint.

This sparse reward setup mimics real-world penetration testing, where only strategically correct sequences of actions lead to measurable success. The agent accumulates reward signals over time and learns the optimal penetration path through TD updates in DQN training.


